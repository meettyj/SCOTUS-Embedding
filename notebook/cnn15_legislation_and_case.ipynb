{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which device are we in:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_checkPoint_path = '../ModelCheckpoint/cnn15_leg_and_case/'\n",
    "model_checkPoint_file_name = 'cnn15_leg_and_case'\n",
    "EPOCH_NUM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "MAX_NB_WORDS = 170000\n",
    "EMBEDDING_DIM = 300\n",
    "learning_rate = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('which device are we in: ', device) # cuda:0 means we do have a gpu\n",
    "# create saved path\n",
    "if not os.path.exists(model_checkPoint_path):\n",
    "    os.makedirs(model_checkPoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors ...\n",
      "Found 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained embedding\n",
    "print('Indexing word vectors ...')\n",
    "embeddings_index = {}\n",
    "embedding_path = '../data/GoogleNews-vectors-negative300.txt'\n",
    "# f = file_io.FileIO('../data/GoogleNews-vectors-negative300.txt', mode='r')\n",
    "with open(embedding_path, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process citation: legislation and case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_caseName_and_citeID(feature_record):\n",
    "    # process case name\n",
    "    case_name_record = feature_record['case_name']\n",
    "    if case_name_record == None:\n",
    "        print('We do find a None in case_name')\n",
    "    if '@' in case_name_record:\n",
    "        print('We do find a @')\n",
    "        \n",
    "    # process cite id\n",
    "    cite_id_record = feature_record['us_cite_id']\n",
    "    if cite_id_record == None:\n",
    "        cite_id_record = 'None'\n",
    "    if '@' in cite_id_record:\n",
    "        print('We do find a @')\n",
    "    \n",
    "    # prepare for the dictionary key\n",
    "    output_dict_key = case_name_record+'@'+cite_id_record\n",
    "    return output_dict_key\n",
    "\n",
    "# Read from the saved mapping file\n",
    "def get_caseName_and_citeID_to_savedID_dict():\n",
    "    saved_mapping_path = '/misc/grice1/yijun/SCOTUS-Embedding/data/'\n",
    "    saved_mapping_file_name = 'caseName_and_citeID_to_savedID.txt'\n",
    "\n",
    "    # read the saved mapping file\n",
    "    with open(saved_mapping_path + saved_mapping_file_name, 'r') as f:\n",
    "        read_list = f.read().split('\\n')[:-1]\n",
    "\n",
    "    print(type(read_list))\n",
    "    print('length of read_list: ', len(read_list))\n",
    "\n",
    "    # build a dict to map case from 8K dateset to the file name\n",
    "    read_dict = {}\n",
    "    for line in read_list:\n",
    "        case_name, cite_id, file_name = line.split('@')\n",
    "        key = case_name + '@' + cite_id\n",
    "        read_dict[key] = file_name\n",
    "        \n",
    "    return read_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_UWash_leg_and_case = '/misc/grice1/meyers/appellate_courts/UWash_scotus_output/'\n",
    "def replace_leg_and_case(file_id, text_before_processed):\n",
    "    leg_file = path_UWash_leg_and_case + file_id + '.legislation10'\n",
    "    case_file = path_UWash_leg_and_case + file_id + '.case10'\n",
    "    start_end_index_leg_and_case_dict = {}\n",
    "    \n",
    "    # legislation file\n",
    "    with open(leg_file, 'r') as leg_file_reader:\n",
    "        text = leg_file_reader.read()\n",
    "        tree = ET.fromstring(\"<root>\" + text + \"</root>\")  # deal with multiple root node\n",
    "        tree = ET.ElementTree(tree)\n",
    "\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            if child.tag == 'citation':\n",
    "                temp_attrib = child.attrib\n",
    "                print(temp_attrib)\n",
    "                temp_start = int(temp_attrib['start'])\n",
    "                temp_end = int(temp_attrib['end'])\n",
    "                print(temp_start)\n",
    "                print(temp_end)\n",
    "                print('segmented text: ', text_before_processed[temp_start:temp_end])\n",
    "                start_end_index_leg_and_case_dict[(temp_start, temp_end)] = 'legislation'\n",
    "    \n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "    # case file\n",
    "    with open(case_file, 'r') as case_file_reader:\n",
    "        text = case_file_reader.read()\n",
    "        tree = ET.fromstring(\"<root>\" + text + \"</root>\")  # deal with multiple root node\n",
    "        tree = ET.ElementTree(tree)\n",
    "\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            if child.tag == 'citation':\n",
    "                temp_attrib = child.attrib\n",
    "                print(temp_attrib)\n",
    "                temp_start = int(temp_attrib['start'])\n",
    "                temp_end = int(temp_attrib['end'])\n",
    "                print(temp_start)\n",
    "                print(temp_end)\n",
    "                print('segmented text: ', text_before_processed[temp_start:temp_end])\n",
    "                start_end_index_leg_and_case_dict[(temp_start, temp_end)] = 'case'\n",
    "    \n",
    "    # sort the dict\n",
    "    start_end_index_leg_and_case_dict = sorted(start_end_index_leg_and_case_dict.items(), key=lambda pair:pair[0][0], reverse=True)\n",
    "    print('start_end_index_leg_and_case_dict: ', start_end_index_leg_and_case_dict)\n",
    "    \n",
    "    print()\n",
    "    print(type(start_end_index_leg_and_case_dict[0]))\n",
    "    print(type(start_end_index_leg_and_case_dict[0][0]))\n",
    "    \n",
    "#     return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "<class 'list'>\n",
      "length of read_list:  8419\n",
      "{'id': '0000_1', 'entry_type': 'regulation', 'start': '981', 'end': '995', 'line': '4', 'agency': 'U.S.C.A.', 'title': '35', 'section': '33'}\n",
      "981\n",
      "995\n",
      "segmented text:  35 U.S.C.A. 33\n",
      "{'id': '0000_2', 'entry_type': 'act_treaty_code_rule', 'start': '12953', 'end': '12967', 'line': '21', 'document': 'the Patent Act'}\n",
      "12953\n",
      "12967\n",
      "segmented text:  the Patent Act\n",
      "{'id': '0000_3', 'entry_type': 'regulation', 'start': '19146', 'end': '19158', 'line': '29', 'agency': 'U.S.C.', 'title': '35', 'section': '33'}\n",
      "19146\n",
      "19158\n",
      "segmented text:  35 U.S.C. 33\n",
      "{'id': '0000_4', 'entry_type': 'regulation', 'start': '19160', 'end': '19174', 'line': '29', 'agency': 'U.S.C.A.', 'title': '35', 'section': '33'}\n",
      "19160\n",
      "19174\n",
      "segmented text:  35 U.S.C.A. 33\n",
      "--------------------------------------------------\n",
      "{'id': '0000_1', 'entry_type': 'case_X_vs_Y', 'start': '2', 'end': '114', 'party1': 'Halliburton Oil Well Cementing Co.', 'party2': 'Walker Mr.Earl Babcock, of Duncan, Okl. (Harry C. Robb, of Washington, D.C', 'line': '1'}\n",
      "2\n",
      "114\n",
      "segmented text:  Halliburton Oil Well Cementing Co. v. Walker Mr.Earl Babcock, of Duncan, Okl. (Harry C. Robb, of Washington, D.C\n",
      "{'id': '0000_13', 'entry_type': 'standard_case', 'start': '672', 'end': '684', 'reporter': 'F.2d', 'standard_reporter': 'F.2D', 'volume': '146', 'page_number': '817', 'line': '4'}\n",
      "672\n",
      "684\n",
      "segmented text:  146 F.2d 817\n",
      "{'id': '0000_14', 'entry_type': 'standard_case', 'start': '735', 'end': '747', 'reporter': 'F.2d', 'standard_reporter': 'F.2D', 'volume': '149', 'page_number': '896', 'line': '4'}\n",
      "735\n",
      "747\n",
      "segmented text:  149 F.2d 896\n",
      "{'id': '0000_15', 'entry_type': 'case_X_vs_Y', 'start': '1039', 'end': '1100', 'party1': 'General Electric Co.', 'party2': 'Wabash Appliance Corporation, .2 This', 'line': '4'}\n",
      "1039\n",
      "1100\n",
      "segmented text:  General Electric Co. v. Wabash Appliance Corporation, .2 This\n",
      "{'id': '0000_34', 'entry_type': 'case_X_vs_Y', 'start': '2089', 'end': '2143', 'party1': 'Railroad Commission of Texas', 'party2': 'Rowan & Nichols Oil Co', 'line': '6'}\n",
      "2089\n",
      "2143\n",
      "segmented text:  Railroad Commission of Texas v. Rowan & Nichols Oil Co\n",
      "{'id': '0000_35', 'entry_type': 'case_X_vs_Y', 'start': '2148', 'end': '2176', 'party1': 'Burford', 'party2': 'Sun Oil Co., . It', 'line': '6'}\n",
      "2148\n",
      "2176\n",
      "segmented text:  Burford v. Sun Oil Co., . It\n",
      "{'id': '0000_73', 'entry_type': 'case_X_vs_Y', 'start': '9270', 'end': '9322', 'party1': 'General Electric Co.', 'party2': 'Wabash Appliance Corporation', 'line': '14'}\n",
      "9270\n",
      "9322\n",
      "segmented text:  General Electric Co. v. Wabash Appliance Corporation\n",
      "{'id': '0000_92', 'entry_type': 'case_X_vs_Y', 'start': '10877', 'end': '10917', 'party1': 'Holland Furniture Co.', 'party2': 'Perkins Glue Co', 'line': '18'}\n",
      "10877\n",
      "10917\n",
      "segmented text:  Holland Furniture Co. v. Perkins Glue Co\n",
      "{'id': '0000_91', 'entry_type': 'standard_case', 'start': '10930', 'end': '10947', 'reporter': 'S.Ct.', 'standard_reporter': 'S.CT.', 'volume': '48', 'page_number': '474', 'paragraph_number': '478', 'line': '18'}\n",
      "10930\n",
      "10947\n",
      "segmented text:  48 S.Ct. 474, 478\n",
      "{'id': '0000_93', 'entry_type': 'case_X_vs_Y', 'start': '10954', 'end': '11006', 'party1': 'General Electric Co.', 'party2': 'Wabash Appliance Corporation', 'line': '18'}\n",
      "10954\n",
      "11006\n",
      "segmented text:  General Electric Co. v. Wabash Appliance Corporation\n",
      "{'id': '0000_106', 'entry_type': 'case_X_vs_Y', 'start': '11694', 'end': '11718', 'party1': 'Corning et al.', 'party2': 'Burden', 'line': '19'}\n",
      "11694\n",
      "11718\n",
      "segmented text:  Corning et al. v. Burden\n",
      "{'id': '0000_105', 'entry_type': 'standard_case', 'start': '11720', 'end': '11736', 'reporter': 'How.', 'standard_reporter': 'HOW.', 'volume': '15', 'page_number': '252', 'paragraph_number': '267', 'line': '19'}\n",
      "11720\n",
      "11736\n",
      "segmented text:  15 How. 252, 267\n",
      "{'id': '0000_115', 'entry_type': 'case_X_vs_Y', 'start': '12324', 'end': '12389', 'party1': 'Lincoln Engineering Co. of Illinois', 'party2': 'Stewart Warner Corporation', 'line': '20'}\n",
      "12324\n",
      "12389\n",
      "segmented text:  Lincoln Engineering Co. of Illinois v. Stewart Warner Corporation\n",
      "{'id': '0000_113', 'entry_type': 'standard_case', 'start': '12402', 'end': '12414', 'reporter': 'S.Ct.', 'standard_reporter': 'S.CT.', 'volume': '58', 'page_number': '662', 'line': '20'}\n",
      "12402\n",
      "12414\n",
      "segmented text:  58 S.Ct. 662\n",
      "{'id': '0000_116', 'entry_type': 'case_X_vs_Y', 'start': '12555', 'end': '12568', 'party1': 'Gill', 'party2': 'Wells', 'line': '20'}\n",
      "12555\n",
      "12568\n",
      "segmented text:  Gill v. Wells\n",
      "{'id': '0000_114', 'entry_type': 'standard_case', 'start': '12570', 'end': '12584', 'reporter': 'Wall.', 'standard_reporter': 'WALL.REP.', 'volume': '22', 'page_number': '1', 'paragraph_number': '28', 'line': '20'}\n",
      "12570\n",
      "12584\n",
      "segmented text:  22 Wall. 1, 28\n",
      "{'id': '0000_117', 'entry_type': 'case_X_vs_Y', 'start': '12590', 'end': '12607', 'party1': 'Fuller', 'party2': 'Yentzer', 'line': '20'}\n",
      "12590\n",
      "12607\n",
      "segmented text:  Fuller v. Yentzer\n",
      "{'id': '0000_118', 'entry_type': 'case_X_vs_Y', 'start': '12809', 'end': '12822', 'party1': 'Gill', 'party2': 'Wells', 'line': '20'}\n",
      "12809\n",
      "12822\n",
      "segmented text:  Gill v. Wells\n",
      "{'id': '0000_139', 'entry_type': 'case_X_vs_Y', 'start': '14028', 'end': '14041', 'party1': 'Gill', 'party2': 'Wells', 'line': '22'}\n",
      "14028\n",
      "14041\n",
      "segmented text:  Gill v. Wells\n",
      "{'id': '0000_145', 'entry_type': 'case_X_vs_Y', 'start': '14119', 'end': '14137', 'party1': 'Merrill', 'party2': 'Yeomans', 'line': '23'}\n",
      "14119\n",
      "14137\n",
      "segmented text:  Merrill v. Yeomans\n",
      "{'id': '0000_146', 'entry_type': 'case_X_vs_Y', 'start': '14441', 'end': '14493', 'party1': 'General Electric Co.', 'party2': 'Wabash Appliance Corporation', 'line': '23'}\n",
      "14441\n",
      "14493\n",
      "segmented text:  General Electric Co. v. Wabash Appliance Corporation\n",
      "{'id': '0000_157', 'entry_type': 'case_X_vs_Y', 'start': '16066', 'end': '16116', 'party1': 'General Electric Co.', 'party2': 'Jewel Incandescent Lamp Co', 'line': '24'}\n",
      "16066\n",
      "16116\n",
      "segmented text:  General Electric Co. v. Jewel Incandescent Lamp Co\n",
      "{'id': '0000_176', 'entry_type': 'case_X_vs_Y', 'start': '17161', 'end': '17206', 'party1': 'United Carbon Co. et al.', 'party2': 'Binney & Smith Co', 'line': '25'}\n",
      "17161\n",
      "17206\n",
      "segmented text:  United Carbon Co. et al. v. Binney & Smith Co\n",
      "{'id': '0000_177', 'entry_type': 'case_X_vs_Y', 'start': '17211', 'end': '17225', 'party1': 'Burr', 'party2': 'Duryee', 'line': '25'}\n",
      "17211\n",
      "17225\n",
      "segmented text:  Burr v. Duryee\n",
      "{'id': '0000_174', 'entry_type': 'standard_case', 'start': '17227', 'end': '17243', 'reporter': 'Wall.', 'standard_reporter': 'WALL.REP.', 'volume': '1', 'page_number': '531', 'paragraph_number': '568', 'line': '25'}\n",
      "17227\n",
      "17243\n",
      "segmented text:  1 Wall. 531, 568\n",
      "{'id': '0000_178', 'entry_type': 'case_X_vs_Y', 'start': '17245', 'end': '17275', 'party1': \"O'Reilly et al.\", 'party2': 'Morse et al', 'line': '25'}\n",
      "17245\n",
      "17275\n",
      "segmented text:  O'Reilly et al. v. Morse et al\n",
      "{'id': '0000_175', 'entry_type': 'standard_case', 'start': '17278', 'end': '17293', 'reporter': 'How.', 'standard_reporter': 'HOW.', 'volume': '15', 'page_number': '62', 'paragraph_number': '112', 'line': '25'}\n",
      "17278\n",
      "17293\n",
      "segmented text:  15 How. 62, 112\n",
      "{'id': '0000_191', 'entry_type': 'case_citation_other', 'start': '17545', 'end': '17560', 'name': \"Walker's patent\", 'line': '25'}\n",
      "17545\n",
      "17560\n",
      "segmented text:  Walker's patent\n",
      "{'id': '0000_179', 'entry_type': 'case_X_vs_Y', 'start': '18219', 'end': '18236', 'party1': 'Fuller', 'party2': 'Yentzer', 'line': '25'}\n",
      "18219\n",
      "18236\n",
      "segmented text:  Fuller v. Yentzer\n",
      "{'id': '0000_180', 'entry_type': 'case_X_vs_Y', 'start': '18262', 'end': '18275', 'party1': 'Gill', 'party2': 'Wells', 'line': '25'}\n",
      "18262\n",
      "18275\n",
      "segmented text:  Gill v. Wells\n",
      "{'id': '0000_217', 'entry_type': 'case_X_vs_Y', 'start': '18559', 'end': '18608', 'party1': 'Continental Paper Bag Co.', 'party2': 'Eastern Paper Bag Co', 'line': '26'}\n",
      "18559\n",
      "18608\n",
      "segmented text:  Continental Paper Bag Co. v. Eastern Paper Bag Co\n",
      "{'id': '0000_234', 'entry_type': 'case_X_vs_Y', 'start': '21343', 'end': '21363', 'party1': 'Hailes', 'party2': 'Van Wormer', 'line': '42'}\n",
      "21343\n",
      "21363\n",
      "segmented text:  Hailes v. Van Wormer\n",
      "{'id': '0000_230', 'entry_type': 'standard_case', 'start': '21365', 'end': '21377', 'reporter': 'Wall.', 'standard_reporter': 'WALL.REP.', 'volume': '20', 'page_number': '353', 'line': '42'}\n",
      "21365\n",
      "21377\n",
      "segmented text:  20 Wall. 353\n",
      "{'id': '0000_235', 'entry_type': 'case_X_vs_Y', 'start': '21379', 'end': '21393', 'party1': 'Knapp', 'party2': 'Morss', 'line': '42'}\n",
      "21379\n",
      "21393\n",
      "segmented text:  Knapp v. Morss\n",
      "{'id': '0000_231', 'entry_type': 'standard_case', 'start': '21405', 'end': '21420', 'reporter': 'S.Ct.', 'standard_reporter': 'S.CT.', 'volume': '14', 'page_number': '81', 'paragraph_number': '83', 'line': '42'}\n",
      "21405\n",
      "21420\n",
      "segmented text:  14 S.Ct. 81, 83\n",
      "{'id': '0000_236', 'entry_type': 'case_X_vs_Y', 'start': '21426', 'end': '21480', 'party1': 'Textile Machine Works', 'party2': 'Louis Hirsch Textile Machines', 'line': '42'}\n",
      "21426\n",
      "21480\n",
      "segmented text:  Textile Machine Works v. Louis Hirsch Textile Machines\n",
      "{'id': '0000_232', 'entry_type': 'standard_case', 'start': '21490', 'end': '21503', 'reporter': 'S. Ct.', 'standard_reporter': 'S.CT.', 'volume': '58', 'page_number': '291', 'line': '42'}\n",
      "21490\n",
      "21503\n",
      "segmented text:  58 S. Ct. 291\n",
      "{'id': '0000_237', 'entry_type': 'case_X_vs_Y', 'start': '21505', 'end': '21574', 'party1': 'Lincoln Engineering Co. of Illinois', 'party2': 'Stewart- Warner Corp., , 550 S', 'line': '42'}\n",
      "21505\n",
      "21574\n",
      "segmented text:  Lincoln Engineering Co. of Illinois v. Stewart- Warner Corp., , 550 S\n",
      "{'id': '0000_233', 'entry_type': 'standard_case', 'start': '21577', 'end': '21594', 'reporter': 'S.Ct.', 'standard_reporter': 'S.CT.', 'volume': '58', 'page_number': '662', 'paragraph_number': '664', 'line': '42'}\n",
      "21577\n",
      "21594\n",
      "segmented text:  58 S.Ct. 662, 664\n",
      "{'id': '0000_266', 'entry_type': 'case_X_vs_Y', 'start': '23820', 'end': '23869', 'party1': 'Continental Paper Bag Co.', 'party2': 'Eastern Paper Bag Co', 'line': '48'}\n",
      "23820\n",
      "23869\n",
      "segmented text:  Continental Paper Bag Co. v. Eastern Paper Bag Co\n",
      "{'id': '0000_265', 'entry_type': 'standard_case', 'start': '23885', 'end': '23902', 'reporter': 'S.Ct.', 'standard_reporter': 'S.CT.', 'volume': '28', 'page_number': '748', 'paragraph_number': '750', 'line': '48'}\n",
      "23885\n",
      "23902\n",
      "segmented text:  28 S.Ct. 748, 750\n",
      "start_end_index_leg_and_case_dict:  [((23885, 23902), 'case'), ((23820, 23869), 'case'), ((21577, 21594), 'case'), ((21505, 21574), 'case'), ((21490, 21503), 'case'), ((21426, 21480), 'case'), ((21405, 21420), 'case'), ((21379, 21393), 'case'), ((21365, 21377), 'case'), ((21343, 21363), 'case'), ((19160, 19174), 'legislation'), ((19146, 19158), 'legislation'), ((18559, 18608), 'case'), ((18262, 18275), 'case'), ((18219, 18236), 'case'), ((17545, 17560), 'case'), ((17278, 17293), 'case'), ((17245, 17275), 'case'), ((17227, 17243), 'case'), ((17211, 17225), 'case'), ((17161, 17206), 'case'), ((16066, 16116), 'case'), ((14441, 14493), 'case'), ((14119, 14137), 'case'), ((14028, 14041), 'case'), ((12953, 12967), 'legislation'), ((12809, 12822), 'case'), ((12590, 12607), 'case'), ((12570, 12584), 'case'), ((12555, 12568), 'case'), ((12402, 12414), 'case'), ((12324, 12389), 'case'), ((11720, 11736), 'case'), ((11694, 11718), 'case'), ((10954, 11006), 'case'), ((10930, 10947), 'case'), ((10877, 10917), 'case'), ((9270, 9322), 'case'), ((2148, 2176), 'case'), ((2089, 2143), 'case'), ((1039, 1100), 'case'), ((981, 995), 'legislation'), ((735, 747), 'case'), ((672, 684), 'case'), ((2, 114), 'case')]\n",
      "\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "Found 1 texts.\n",
      "Found 15 labels.\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "sc = SupremeCourt()\n",
    "print(sc.info)\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "issue_codes.sort()\n",
    "issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "\n",
    "# get mapping dict between UWash dataset and local file name (id)\n",
    "caseName_and_citeID_to_savedID_dict = get_caseName_and_citeID_to_savedID_dict()\n",
    "\n",
    "for record in sc.records():\n",
    "    text_record = record[0]\n",
    "    feature_record = record[1]\n",
    "    \n",
    "    if feature_record['issue'] == None: # some cases have None as an issue\n",
    "        labels.append(labels_index['-1'])\n",
    "    else:\n",
    "        labels.append(labels_index[feature_record['issue'][:-4]])\n",
    "    \n",
    "    # process citation: legislation and case\n",
    "    \n",
    "    # get file id\n",
    "    dict_key = get_record_caseName_and_citeID(feature_record)\n",
    "    file_id = caseName_and_citeID_to_savedID_dict[dict_key]\n",
    "    \n",
    "    # process citation: legislation and case\n",
    "    replace_leg_and_case(file_id, text_record)\n",
    "    \n",
    "    \n",
    "    texts.append(text_record)\n",
    "    \n",
    "    break\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s labels.' % len(labels_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare train/dev/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173087 unique tokens.\n",
      "Shape of padded_data ndarray: (8419, 90018)\n",
      "Shape of label ndarray: (8419, 15)\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# with tf.device('/gpu:0'):\n",
    "padded_data = pad_sequences(sequences)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = padded_data.shape[1]\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "    \n",
    "print('Shape of padded_data ndarray:', padded_data.shape)\n",
    "print('Shape of label ndarray:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix -> tensor\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "x_train_ndarray, x_test_ndarray, y_train_ndarray, y_test_ndarray = train_test_split(padded_data, labels, test_size=VALIDATION_SPLIT, random_state=42)\n",
    "x_train_ndarray, x_val_ndarray, y_train_ndarray, y_val_ndarray = train_test_split(x_train_ndarray, y_train_ndarray, test_size=VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "# to tensor and to gpu\n",
    "x_train = torch.from_numpy(x_train_ndarray).to(device, dtype=torch.long)\n",
    "# x_train = x_train.unsqueeze(1)\n",
    "y_train = torch.from_numpy(y_train_ndarray).to(device, dtype=torch.long)\n",
    "\n",
    "x_val = torch.from_numpy(x_val_ndarray).to(device, dtype=torch.long)\n",
    "# x_val = x_val.unsqueeze(1)\n",
    "y_val = torch.from_numpy(y_val_ndarray).to(device, dtype=torch.long)\n",
    "\n",
    "x_test = torch.from_numpy(x_test_ndarray).to(device, dtype=torch.long)\n",
    "# x_test = x_test.unsqueeze(1)\n",
    "y_test = torch.from_numpy(y_test_ndarray).to(device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataset_train = data.TensorDataset(x_train,y_train)\n",
    "dataloader_train = data.DataLoader(dataset_train, batch_size=4, shuffle=True)\n",
    "dataset_val = data.TensorDataset(x_val,y_val)\n",
    "dataloader_val = data.DataLoader(dataset_val, batch_size=4, shuffle=False)\n",
    "dataset_test = data.TensorDataset(x_test,y_test)\n",
    "dataloader_test = data.DataLoader(dataset_test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 90018])\n",
      "torch.Size([4, 15])\n"
     ]
    }
   ],
   "source": [
    "dataiter_temp = iter(dataloader_train)\n",
    "images_temp, labels_temp = dataiter_temp.next()\n",
    "# images_temp = images_temp.unsqueeze(1)\n",
    "print(images_temp.size())\n",
    "print(labels_temp.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_words, EMBEDDING_DIM)\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv_module = nn.Sequential(\n",
    "            \n",
    "            nn.Conv1d(300,128,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(128,128,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(128,128,5),\n",
    "            nn.ReLU(),\n",
    "#             nn.MaxPool1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "                \n",
    "        )\n",
    "        \n",
    "        self.dense_module = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(128, len(labels_index)),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.conv_module(x)\n",
    "#         print('after conv: ', x.size())\n",
    "        x, _  = torch.max(x, 2) # global max pooling\n",
    "#         print('after max: ', x.size())\n",
    "        x = self.dense_module(x)\n",
    "        \n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(170000, 300)\n",
       "  (conv_module): Sequential(\n",
       "    (0): Conv1d(300, 128, kernel_size=(5,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "    (4): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.25, inplace=False)\n",
       "    (8): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
       "    (9): ReLU()\n",
       "    (10): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (dense_module): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, outputs = torch.max(outputs, 1) # get the class index\n",
    "        _, labels = torch.max(labels, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, epoch, best):\n",
    "    if best == False:\n",
    "        torch.save(model.state_dict(), \n",
    "                   model_checkPoint_path+model_checkPoint_file_name+'_epoch.pth')\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \n",
    "                   model_checkPoint_path+model_checkPoint_file_name+'_best.pth')\n",
    "    print('Model saved. Epoch: %d, Best: %r' % (epoch, best))\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   400] loss: 0.436\n",
      "[1,   800] loss: 0.391\n",
      "[1,  1200] loss: 0.362\n",
      "[1,  1600] loss: 0.347\n",
      "-------------- Evaluating --------------\n",
      "Evaluation Accuracy: 43.139842 %\n",
      "Model saved. Epoch: 0, Best: True\n",
      "Model saved. Epoch: 0, Best: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-2ccde77c8004>\", line 30, in <module>\n",
      "    running_loss += loss.item()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/tensorflow_core/contrib/__init__.py\", line 40, in <module>\n",
      "    from tensorflow.contrib import constrained_optimization\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/tensorflow_core/contrib/constrained_optimization/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.contrib.constrained_optimization.python.candidates import *\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/tensorflow_core/contrib/constrained_optimization/__init__.py\", line 26, in <module>\n",
      "    from tensorflow.contrib.constrained_optimization.python.swap_regret_optimizer import *\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/site-packages/tensorflow_core/contrib/constrained_optimization/python/swap_regret_optimizer.py\", line 221, in <module>\n",
      "    class _SwapRegretOptimizer(constrained_optimizer.ConstrainedOptimizer):\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/abc.py\", line 145, in __new__\n",
      "    cls._abc_registry = WeakSet()\n",
      "  File \"/misc/grice1/yijun/miniconda3/envs/yijun/lib/python3.6/_weakrefset.py\", line 48, in __init__\n",
      "    self._iterating = set()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "best_accuracy_val = 0\n",
    "epochs_of_best_models_list = []\n",
    "\n",
    "for epoch in range(EPOCH_NUM):  # loop over the dataset multiple times\n",
    "    \n",
    "    # train\n",
    "    running_loss = 0.0\n",
    "    for i, data_train in enumerate(dataloader_train, 0):\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs_train, labels_train = data_train\n",
    "#         print(inputs_train.size())\n",
    "#         print(labels_train.size())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs_train = model(inputs_train)\n",
    "#         print(outputs_train.size())\n",
    "#         print(labels_train.size())\n",
    "#         print(torch.max(labels_train, 1)[1])\n",
    "#         print('outputs_train: ', outputs_train)\n",
    "        \n",
    "        loss = criterion(outputs_train, torch.max(labels_train, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 400 == 0:    # print every 400 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # validation\n",
    "    if epoch % 1 == 0: # may change if needed\n",
    "        print('-------------- Evaluating --------------')\n",
    "        accuracy_val = get_accuracy(model, dataloader_val)\n",
    "        print('Evaluation Accuracy: %f %%' % (accuracy_val))      \n",
    "        if accuracy_val > best_accuracy_val:\n",
    "            best_accuracy_val = accuracy_val\n",
    "            epoch_of_best_model = save_model(model, epoch, best=True)\n",
    "            epochs_of_best_models_list.append(epoch_of_best_model)\n",
    "        if epoch % 5 == 0: # save the model every 5 epochs.\n",
    "            save_model(model, epoch, best=False)\n",
    "        \n",
    "print('Finished Training')\n",
    "print('epochs_of_best_models_list (latest last): ', epochs_of_best_models_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------- testing --------------')\n",
    "# load best model\n",
    "best_model = Net().to(device)\n",
    "best_model.load_state_dict(torch.load(model_checkPoint_path+model_checkPoint_file_name+'_best.pth', \n",
    "                                      map_location=device))\n",
    "accuracy_test = get_accuracy(best_model, dataloader_test)\n",
    "print('Testing Accuracy: %f %%' % (accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
