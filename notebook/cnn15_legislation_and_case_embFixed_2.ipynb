{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which device are we in:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_checkPoint_path = '../ModelCheckpoint/cnn15_leg_and_case_temp/'\n",
    "model_checkPoint_file_name = 'cnn15_temp'\n",
    "EPOCH_NUM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "MAX_NB_WORDS = 170000\n",
    "EMBEDDING_DIM = 300\n",
    "learning_rate = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('which device are we in: ', device) # cuda:0 means we do have a gpu\n",
    "# create saved path\n",
    "if not os.path.exists(model_checkPoint_path):\n",
    "    os.makedirs(model_checkPoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors ...\n",
      "Found 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained embedding\n",
    "print('Indexing word vectors ...')\n",
    "embeddings_index = {}\n",
    "embedding_path = '../data/GoogleNews-vectors-negative300.txt'\n",
    "# f = file_io.FileIO('../data/GoogleNews-vectors-negative300.txt', mode='r')\n",
    "with open(embedding_path, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Processing text dataset')\n",
    "\n",
    "# sc = SupremeCourt()\n",
    "# print(sc.info)\n",
    "\n",
    "# texts = []  # list of text samples\n",
    "# labels_index = {}  # dictionary mapping label name to numeric id\n",
    "# labels = []  # list of label ids\n",
    "\n",
    "# issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "# issue_codes.sort()\n",
    "# issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "# labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "\n",
    "# for record in sc.records():\n",
    "#     if record[1]['issue'] == None: # some cases have None as an issue\n",
    "#         labels.append(labels_index['-1'])\n",
    "#     else:\n",
    "#         labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "#     texts.append(record[0])\n",
    "\n",
    "# print('Found %s texts.' % len(texts))\n",
    "# print('Found %s labels.' % len(labels_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_caseName_and_citeID(feature_record):\n",
    "    # process case name\n",
    "    case_name_record = feature_record['case_name']\n",
    "    if case_name_record == None:\n",
    "        print('We do find a None in case_name')\n",
    "    if '@' in case_name_record:\n",
    "        print('We do find a @')\n",
    "        \n",
    "    # process cite id\n",
    "    cite_id_record = feature_record['us_cite_id']\n",
    "    if cite_id_record == None:\n",
    "        cite_id_record = 'None'\n",
    "    if '@' in cite_id_record:\n",
    "        print('We do find a @')\n",
    "    \n",
    "    # prepare for the dictionary key\n",
    "    output_dict_key = case_name_record+'@'+cite_id_record\n",
    "    return output_dict_key\n",
    "\n",
    "# Read from the saved mapping file\n",
    "def get_caseName_and_citeID_to_savedID_dict():\n",
    "    saved_mapping_path = '/misc/grice1/yijun/SCOTUS-Embedding/data/'\n",
    "    saved_mapping_file_name = 'caseName_and_citeID_to_savedID.txt'\n",
    "\n",
    "    # read the saved mapping file\n",
    "    with open(saved_mapping_path + saved_mapping_file_name, 'r') as f:\n",
    "        read_list = f.read().split('\\n')[:-1]\n",
    "\n",
    "    # print(type(read_list))\n",
    "    # print('length of read_list: ', len(read_list))\n",
    "\n",
    "    # build a dict to map case from 8K dateset to the file name\n",
    "    read_dict = {}\n",
    "    for line in read_list:\n",
    "        case_name, cite_id, file_name = line.split('@')\n",
    "        key = case_name + '@' + cite_id\n",
    "        read_dict[key] = file_name\n",
    "        \n",
    "    return read_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove single quote\n",
    "def fix_format_problem_for_specific_file(file_id, text):\n",
    "    if file_id == '3658': # to fix the & problem at line 2 column 123\n",
    "        return text.replace('.\" ', '.  ') \n",
    "    elif file_id == '4119': \n",
    "        return text.replace('.\" 1', '.  1')\n",
    "    elif file_id == '6349': \n",
    "        return text.replace('.\" 1', '.  1').replace('.\" B', '.  B')\n",
    "    elif file_id == '7072': \n",
    "        return text.replace('.].\"', '.]. ')\n",
    "    elif file_id == '7103': \n",
    "        return text.replace('Act.\"', 'Act. ')\n",
    "    elif file_id == '7578': \n",
    "        return text.replace('York.\"', 'York. ')\n",
    "    elif file_id == '7835': \n",
    "        return text.replace('\");', ' );')\n",
    "    elif file_id == '8244': \n",
    "        return text.replace('little\" ', 'little  ')\n",
    "    elif file_id == '8294': \n",
    "        return text.replace('county.\"', 'county. ')\n",
    "    elif file_id == '8311': \n",
    "        return text.replace('dismissal\"', 'dismissal ')\n",
    "    return text \n",
    "\n",
    "def fix_format_problems(file_id, text):\n",
    "    text = text.replace('& ', '  ') # to fix the & problem at line 10 column 119 in file 1973.legislation10\n",
    "    text = text.replace('... .\"', '... . ') # to fix the & problem at line 16 column 148 in file 3009.legislation10\n",
    "    return fix_format_problem_for_specific_file(file_id, text)\n",
    "\n",
    "def replace_sub_string(input_string, start, end, input_class):\n",
    "    return input_string[:start] + input_class + input_string[end:]\n",
    "\n",
    "path_UWash_leg_and_case = '/misc/grice1/meyers/appellate_courts/UWash_scotus_output/'\n",
    "def replace_leg_and_case(file_id, text_before_processed):\n",
    "    leg_file = path_UWash_leg_and_case + file_id + '.legislation10'\n",
    "    case_file = path_UWash_leg_and_case + file_id + '.case10'\n",
    "    start_end_index_leg_and_case_dict = {}\n",
    "    \n",
    "    # legislation file\n",
    "    with open(leg_file, 'r') as leg_file_reader:\n",
    "        # print('processing leg file')\n",
    "        temp_text = leg_file_reader.read()\n",
    "        temp_text = fix_format_problems(file_id, temp_text)\n",
    "        \n",
    "        tree = ET.fromstring(\"<root>\" + temp_text + \"</root>\")  # deal with multiple root node\n",
    "        tree = ET.ElementTree(tree)\n",
    "\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            if child.tag == 'citation':\n",
    "                temp_attrib = child.attrib\n",
    "#                 print(temp_attrib)\n",
    "                temp_start = int(temp_attrib['start'])\n",
    "                temp_end = int(temp_attrib['end'])\n",
    "#                 print(temp_start)\n",
    "#                 print(temp_end)\n",
    "#                 print('segmented text: ', text_before_processed[temp_start:temp_end])\n",
    "                start_end_index_leg_and_case_dict[(temp_start, temp_end)] = 'legislation'\n",
    "    \n",
    "    # case file\n",
    "    with open(case_file, 'r') as case_file_reader:\n",
    "        # print('processing case file')\n",
    "        temp_text = case_file_reader.read()\n",
    "        tree = ET.fromstring(\"<root>\" + temp_text + \"</root>\")  # deal with multiple root node\n",
    "        tree = ET.ElementTree(tree)\n",
    "\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            if child.tag == 'citation':\n",
    "                temp_attrib = child.attrib\n",
    "#                 print(temp_attrib)\n",
    "                temp_start = int(temp_attrib['start'])\n",
    "                temp_end = int(temp_attrib['end'])\n",
    "#                 print(temp_start)\n",
    "#                 print(temp_end)\n",
    "#                 print('segmented text: ', text_before_processed[temp_start:temp_end])\n",
    "                start_end_index_leg_and_case_dict[(temp_start, temp_end)] = 'case'\n",
    "    \n",
    "    # sort the dict\n",
    "    start_end_index_leg_and_case_tuple_list = sorted(start_end_index_leg_and_case_dict.items(), key=lambda pair:pair[0][0], reverse=True)\n",
    "    \n",
    "    text_processed = text_before_processed # initialization\n",
    "    for tuple_pair in start_end_index_leg_and_case_tuple_list:\n",
    "        tuple_start, tuple_end = tuple_pair[0]\n",
    "        tuple_class = tuple_pair[1]\n",
    "        if tuple_class == 'legislation':\n",
    "            text_processed = replace_sub_string(text_processed, tuple_start, tuple_end, 'legislation')\n",
    "        elif tuple_class == 'case':\n",
    "            text_processed = replace_sub_string(text_processed, tuple_start, tuple_end, 'case')\n",
    "        else:\n",
    "            print('This tuple class is not valid, it can only be case or legislation: ', tuple_class)\n",
    "        \n",
    "    # print('--------------------------------------------------')\n",
    "    \n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "processing the files including citations (leg and case) ...\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "sc = SupremeCourt()\n",
    "print(sc.info)\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "issue_codes.sort()\n",
    "issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "\n",
    "# get mapping dict between UWash dataset and local file name (id)\n",
    "caseName_and_citeID_to_savedID_dict = get_caseName_and_citeID_to_savedID_dict()\n",
    "\n",
    "print('processing the files including citations (leg and case) ...')\n",
    "for record in sc.records():\n",
    "    text_record = record[0]\n",
    "    feature_record = record[1]\n",
    "    \n",
    "#     if feature_record['issue'] == None: # some cases have None as an issue\n",
    "#         labels.append(labels_index['-1'])\n",
    "#     else:\n",
    "#         labels.append(labels_index[feature_record['issue'][:-4]])\n",
    "        \n",
    "    if record[1]['issue'] == None: # some cases have None as an issue\n",
    "        labels.append(labels_index['-1'])\n",
    "    else:\n",
    "        labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "    \n",
    "    # get file id to process leg and case\n",
    "    dict_key = get_record_caseName_and_citeID(feature_record)\n",
    "    file_id = caseName_and_citeID_to_savedID_dict[dict_key]\n",
    "    \n",
    "    # process citation: legislation and case\n",
    "    text_processed = replace_leg_and_case(file_id, text_record)\n",
    "    \n",
    "    # print('text_processed: ', text_processed)\n",
    "    texts.append(text_processed)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s labels.' % len(labels_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# with tf.device('/gpu:0'):\n",
    "padded_data = pad_sequences(sequences)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = padded_data.shape[1]\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "    \n",
    "print('Shape of padded_data ndarray:', padded_data.shape)\n",
    "print('Shape of label ndarray:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix -> tensor\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "x_train_ndarray, x_test_ndarray, y_train_ndarray, y_test_ndarray = train_test_split(padded_data, labels, test_size=VALIDATION_SPLIT, random_state=42)\n",
    "x_train_ndarray, x_val_ndarray, y_train_ndarray, y_val_ndarray = train_test_split(x_train_ndarray, y_train_ndarray, test_size=VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "# to tensor and to gpu\n",
    "x_train = torch.from_numpy(x_train_ndarray).to(device, dtype=torch.long)\n",
    "# x_train = x_train.unsqueeze(1)\n",
    "y_train = torch.from_numpy(y_train_ndarray).to(device, dtype=torch.long)\n",
    "\n",
    "x_val = torch.from_numpy(x_val_ndarray).to(device, dtype=torch.long)\n",
    "# x_val = x_val.unsqueeze(1)\n",
    "y_val = torch.from_numpy(y_val_ndarray).to(device, dtype=torch.long)\n",
    "\n",
    "x_test = torch.from_numpy(x_test_ndarray).to(device, dtype=torch.long)\n",
    "# x_test = x_test.unsqueeze(1)\n",
    "y_test = torch.from_numpy(y_test_ndarray).to(device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataset_train = data.TensorDataset(x_train,y_train)\n",
    "dataloader_train = data.DataLoader(dataset_train, batch_size=4, shuffle=True)\n",
    "dataset_val = data.TensorDataset(x_val,y_val)\n",
    "dataloader_val = data.DataLoader(dataset_val, batch_size=4, shuffle=False)\n",
    "dataset_test = data.TensorDataset(x_test,y_test)\n",
    "dataloader_test = data.DataLoader(dataset_test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter_temp = iter(dataloader_train)\n",
    "images_temp, labels_temp = dataiter_temp.next()\n",
    "# images_temp = images_temp.unsqueeze(1)\n",
    "print(images_temp.size())\n",
    "print(labels_temp.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_words, EMBEDDING_DIM)\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv_module = nn.Sequential(\n",
    "            \n",
    "            nn.Conv1d(300,128,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(128,128,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(128,128,5),\n",
    "            nn.ReLU(),\n",
    "#             nn.MaxPool1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "                \n",
    "        )\n",
    "        \n",
    "        self.dense_module = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(128, len(labels_index)),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.conv_module(x)\n",
    "#         print('after conv: ', x.size())\n",
    "        x, _  = torch.max(x, 2) # global max pooling\n",
    "#         print('after max: ', x.size())\n",
    "        x = self.dense_module(x)\n",
    "        \n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, outputs = torch.max(outputs, 1) # get the class index\n",
    "        _, labels = torch.max(labels, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, epoch, best):\n",
    "    if best == False:\n",
    "        torch.save(model.state_dict(), \n",
    "                   model_checkPoint_path+model_checkPoint_file_name+'_'+str(epoch)+'.pth')\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \n",
    "                   model_checkPoint_path+model_checkPoint_file_name+'_best.pth')\n",
    "    print('Model saved. Epoch: %d, Best: %r' % (epoch, best))\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_accuracy_val = 0\n",
    "epochs_of_best_models_list = []\n",
    "\n",
    "for epoch in range(EPOCH_NUM):  # loop over the dataset multiple times\n",
    "    \n",
    "    # train\n",
    "    running_loss = 0.0\n",
    "    for i, data_train in enumerate(dataloader_train, 0):\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs_train, labels_train = data_train\n",
    "#         print(inputs_train.size())\n",
    "#         print(labels_train.size())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs_train = model(inputs_train)\n",
    "#         print(outputs_train.size())\n",
    "#         print(labels_train.size())\n",
    "#         print(torch.max(labels_train, 1)[1])\n",
    "#         print('outputs_train: ', outputs_train)\n",
    "        \n",
    "        loss = criterion(outputs_train, torch.max(labels_train, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 400 == 0:    # print every 400 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # validation\n",
    "    if epoch % 1 == 0: # may change if needed\n",
    "        print('-------------- Evaluating --------------')\n",
    "        accuracy_val = get_accuracy(model, dataloader_val)\n",
    "        print('Evaluation Accuracy: %f %%' % (accuracy_val))      \n",
    "        if accuracy_val > best_accuracy_val:\n",
    "            best_accuracy_val = accuracy_val\n",
    "            epoch_of_best_model = save_model(model, epoch, best=True)\n",
    "            epochs_of_best_models_list.append(epoch_of_best_model)\n",
    "        if epoch % 5 == 0: # save the model every 5 epochs.\n",
    "            save_model(model, epoch, best=False)\n",
    "        \n",
    "print('Finished Training')\n",
    "print('epochs_of_best_models_list (latest last): ', epochs_of_best_models_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------- testing --------------')\n",
    "# load best model\n",
    "best_model = Net().to(device)\n",
    "best_model.load_state_dict(torch.load(model_checkPoint_path+model_checkPoint_file_name+'_best.pth', \n",
    "                                      map_location=device))\n",
    "accuracy_test = get_accuracy(best_model, dataloader_test)\n",
    "print('Testing Accuracy: %f %%' % (accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
