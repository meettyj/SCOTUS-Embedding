{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "# from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc.info:  {'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n"
     ]
    }
   ],
   "source": [
    "sc = SupremeCourt()\n",
    "# sc.download()\n",
    "print('sc.info: ', sc.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.issue_area_codes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-1': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " '10': 10,\n",
       " '11': 11,\n",
       " '12': 12,\n",
       " '13': 13,\n",
       " '14': 14}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15 labels\n",
    "issue_codes_15 = list(sc.issue_area_codes.keys()) \n",
    "issue_codes_15.sort()\n",
    "issue_codes_15 = [str(ic) for ic in issue_codes_15]\n",
    "\n",
    "# 279 labels\n",
    "issue_codes_279 = list(sc.issue_codes.keys()) \n",
    "issue_codes_279.append('-1')\n",
    "issue_codes_279.sort()\n",
    "\n",
    "# dictionary mapping label name to numeric id\n",
    "labels_index_15 = dict(zip(issue_codes_15, np.arange(len(issue_codes_15))))\n",
    "labels_index_279 = dict(zip(issue_codes_279, np.arange(len(issue_codes_279))))\n",
    "labels_index_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- The format of one record ---------\n",
      "length:  2\n",
      "tempRecord[0] is the text:  <class 'str'>\n",
      "tempRecord[1] is the dict:  {'issue': '80180', 'issue_area': 8, 'n_min_votes': 1, 'case_name': 'HALLIBURTON OIL WELL CEMENTING CO. v. WALKER et al., DOING BUSINESS AS DEPTHOGRAPH CO.', 'maj_opinion_author': 78, 'decision_date': '1946-11-18', 'decision_direction': 'liberal', 'n_maj_votes': 8, 'us_cite_id': '329 U.S. 1', 'argument_date': '1946-01-09'}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the format of the data.\n",
    "\n",
    "tempRecord = next(sc.records())\n",
    "type(tempRecord)\n",
    "\n",
    "print('--------- The format of one record ---------')\n",
    "print('length: ', len(tempRecord))\n",
    "print('tempRecord[0] is the text: ', type(tempRecord[0]))\n",
    "print('tempRecord[1] is the dict: ', tempRecord[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8419 texts.\n",
      "Found 15 labels in labels_15.\n",
      "Found 264 labels in labels_279.\n",
      "\n",
      "length of case_name_plus_citeId_list:  8419\n",
      "length of case_name_plus_citeId_set:  8419\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels_15 = []  # list of label ids\n",
    "labels_279 = []\n",
    "\n",
    "case_name_plus_citeId_list = []\n",
    "\n",
    "for record in sc.records():\n",
    "    text_record = record[0]\n",
    "    feature_record = record[1]\n",
    "\n",
    "    # process issue number\n",
    "    issue_record = feature_record['issue']\n",
    "    \n",
    "    # 15 labels\n",
    "    if issue_record == None: # some cases have None as an issue\n",
    "        issue_record_label = labels_index_15['-1']\n",
    "    else:\n",
    "        issue_record_label = labels_index_15[feature_record['issue'][:-4]]\n",
    "    labels_15.append(issue_record_label)\n",
    "    \n",
    "    # 279 labels\n",
    "    if issue_record == None: # some cases have None as an issue\n",
    "        labels_279.append(labels_index_279['-1'])\n",
    "    else:\n",
    "        labels_279.append(labels_index_279[feature_record['issue']])\n",
    "    \n",
    "    # process case name\n",
    "    case_name_record = feature_record['case_name']\n",
    "    if case_name_record == None:\n",
    "        print('We do find a None in case_name')\n",
    "    if '@' in case_name_record:\n",
    "        print('We do find a @')\n",
    "        \n",
    "    # process cite id\n",
    "    cite_id_record = feature_record['us_cite_id']\n",
    "    if cite_id_record == None:\n",
    "        cite_id_record = 'None'\n",
    "    if '@' in cite_id_record:\n",
    "        print('We do find a @')\n",
    "    \n",
    "    # prepare for the dictionary key\n",
    "    case_name_plus_citeId_list.append(case_name_record+'@'+cite_id_record)\n",
    "    \n",
    "    # add texts\n",
    "    texts.append(text_record)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s labels in labels_15.' % len(set(labels_15)))\n",
    "print('Found %s labels in labels_279.' % len(set(labels_279)))\n",
    "\n",
    "print()\n",
    "print('length of case_name_plus_citeId_list: ', len(case_name_plus_citeId_list))\n",
    "case_name_plus_citeId_set = set(case_name_plus_citeId_list)\n",
    "print('length of case_name_plus_citeId_set: ', len(case_name_plus_citeId_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "{'issue': '80180', 'issue_area': 8, 'n_min_votes': 1, 'case_name': 'HALLIBURTON OIL WELL CEMENTING CO. v. WALKER et al., DOING BUSINESS AS DEPTHOGRAPH CO.', 'maj_opinion_author': 78, 'decision_date': '1946-11-18', 'decision_direction': 'liberal', 'n_maj_votes': 8, 'us_cite_id': '329 U.S. 1', 'argument_date': '1946-01-09'}\n"
     ]
    }
   ],
   "source": [
    "for record in sc.records():\n",
    "    print(type(record))\n",
    "    print(len(record))\n",
    "    print(record[1])\n",
    "#     print(record['issue'])\n",
    "    break\n",
    "#         if record['issue'] == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the text, and build a corresponding dictionary in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getStringNumber(input_num):\n",
    "    input_num = str(input_num)\n",
    "    output_num = ''\n",
    "    for i in range(4-len(input_num)):\n",
    "        output_num += '0'\n",
    "    output_num += input_num\n",
    "    return output_num\n",
    "\n",
    "getStringNumber(0)\n",
    "# getStringNumber(1)\n",
    "# getStringNumber(123)\n",
    "# getStringNumber(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDictKey(feature_record):\n",
    "    # process case name\n",
    "    case_name_record = feature_record['case_name']\n",
    "    if case_name_record == None:\n",
    "        print('We do find a None in case_name')\n",
    "    if '@' in case_name_record:\n",
    "        print('We do find a @')\n",
    "        \n",
    "    # process cite id\n",
    "    cite_id_record = feature_record['us_cite_id']\n",
    "    if cite_id_record == None:\n",
    "        cite_id_record = 'None'\n",
    "    if '@' in cite_id_record:\n",
    "        print('We do find a @')\n",
    "    \n",
    "    # prepare for the dictionary key\n",
    "    output_dict_key = case_name_record+'@'+cite_id_record\n",
    "    return output_dict_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  8419\n",
      "length of caseName_and_citeID_to_savedID_list:  8419\n",
      "The mapping info has been saved as:  /misc/grice1/yijun/SCOTUS-Embedding/data/caseName_and_citeID_to_savedID.txt\n"
     ]
    }
   ],
   "source": [
    "saved_mapping_path = '/misc/grice1/yijun/SCOTUS-Embedding/data/'\n",
    "saved_files_path = '/misc/grice1/yijun/SCOTUS-Embedding/data/supreme_court_8K/'\n",
    "count = 0\n",
    "caseName_and_citeID_to_savedID_list = []\n",
    "\n",
    "for record in sc.records():\n",
    "    text_record = record[0]\n",
    "    feature_record = record[1]\n",
    "    \n",
    "    # save to local\n",
    "    saved_string_num = getStringNumber(count)\n",
    "    file_name = saved_string_num + '.txt'\n",
    "    with open(saved_files_path+file_name, 'w') as f:\n",
    "        f.write(text_record)\n",
    "        \n",
    "    # save to list\n",
    "    dict_key = getDictKey(feature_record)\n",
    "    caseName_and_citeID_to_savedID_list.append(dict_key+'@'+saved_string_num)\n",
    "    \n",
    "    count += 1\n",
    "    # break\n",
    "    \n",
    "print('count: ', count)\n",
    "print('length of caseName_and_citeID_to_savedID_list: ', len(caseName_and_citeID_to_savedID_list))\n",
    "\n",
    "# save the mapping list\n",
    "saved_mapping_file_name = 'caseName_and_citeID_to_savedID.txt'\n",
    "saved_mapping_str = ''\n",
    "for i in range(len(caseName_and_citeID_to_savedID_list)):\n",
    "    saved_mapping_str += caseName_and_citeID_to_savedID_list[i]+'\\n'\n",
    "with open(saved_mapping_path + saved_mapping_file_name, 'w') as f:\n",
    "    f.write(saved_mapping_str)\n",
    "print('The mapping info has been saved as: ', saved_mapping_path + saved_mapping_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from the saved mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "length of read_list:  8419\n",
      "\n",
      "we find every case in our dict\n"
     ]
    }
   ],
   "source": [
    "saved_mapping_path = '/misc/grice1/yijun/SCOTUS-Embedding/data/'\n",
    "saved_files_path = '/misc/grice1/yijun/SCOTUS-Embedding/data/supreme_court_8K/'\n",
    "saved_mapping_file_name = 'caseName_and_citeID_to_savedID.txt'\n",
    "\n",
    "# read the saved mapping file\n",
    "with open(saved_mapping_path + saved_mapping_file_name, 'r') as f:\n",
    "    read_list = f.read().split('\\n')[:-1]\n",
    "    \n",
    "print(type(read_list))\n",
    "print('length of read_list: ', len(read_list))\n",
    "\n",
    "# build a dict to map case from 8K dateset to the file name\n",
    "read_dict = {}\n",
    "for line in read_list:\n",
    "    case_name, cite_id, file_name = line.split('@')\n",
    "    key = case_name + '@' + cite_id\n",
    "    read_dict[key] = file_name\n",
    "\n",
    "# go through 8K dataset to find the file name\n",
    "for record in sc.records():\n",
    "    text_record = record[0]\n",
    "    feature_record = record[1]\n",
    "    \n",
    "    dict_key = getDictKey(feature_record)\n",
    "    if dict_key not in read_dict:\n",
    "        print('we find a case from 8K dataset is not in our dict')\n",
    "print('\\nwe find every case in our dict')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the 15 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_labels_path = '/misc/grice1/yijun/SCOTUS-Embedding/data/'\n",
    "saves_labels_file_name = 'UWash_labels.txt'\n",
    "with open(saved_labels_path+saves_labels_file_name, 'w') as f:\n",
    "    output_string = ''\n",
    "    for index in range(len(case_name_plus_citeId_list)):\n",
    "        case_name_plus_citeId = case_name_plus_citeId_list[index]\n",
    "        case_label_15 = str(labels_15[index])\n",
    "        case_label_279 = str(labels_279[index])\n",
    "        file_name = read_dict[case_name_plus_citeId]\n",
    "        output_string = output_string+file_name+'@'+case_label_15+'@'+case_label_279+'\\n'\n",
    "    f.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
